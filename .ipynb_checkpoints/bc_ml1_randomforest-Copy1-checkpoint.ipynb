{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import sklearn as skl\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neural_network\n",
    "from sklearn import neighbors\n",
    "from functools import reduce\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import neighbors\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Data/wharton_ml3.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5114a7f45db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import \"big\" wharton file -> wharton_ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwharton\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/wharton_ml3.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Rename  columns in wharton file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Data/wharton_ml3.csv' does not exist"
     ]
    }
   ],
   "source": [
    "##### MARKET DATA FROM WHARTON\n",
    "\n",
    "# Import \"big\" wharton file -> wharton_ml\n",
    "wharton = pd.read_csv('Data/wharton_ml3.csv', sep = ',')\n",
    "\n",
    "# Rename  columns in wharton file\n",
    "wharton.rename(columns = {'date': 'DATE_w', 'PERMNO': 'PERMNO_w'}, inplace = True)\n",
    "\n",
    "# SPREAD seems to contain no data (nan), so I'm calculating spread via BID - ASK \n",
    "wharton['SPREAD'] = wharton['BID'] - wharton['ASK']\n",
    "\n",
    "# REMOVE DATA THAT IS (PBLY) NOT NEEDED\n",
    "wharton = wharton.drop(['RET', 'RETX', 'DLAMT', 'DLPDT', 'DCLRDT', 'DLRETX', 'DLRET', 'DLPRC', 'PAYDT', 'RCRDDT', \n",
    "                           'SHRFLG', 'DISTCD', 'DIVAMT', 'FACPR', \n",
    "                           'ACPERM', 'ACCOMP', 'SHRENDDT', 'FACSHR', 'ALTPRCDT'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4494bb39eadd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import financial ratios (downloaded from OLAT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mratios_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/Ratios.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'permno'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Rename columns in ratios file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "##### FINANCIAL RATIOS\n",
    "\n",
    "# Import financial ratios (downloaded from OLAT)\n",
    "ratios_all = pd.read_csv('Data/Ratios.csv', sep = ',', converters = {'permno': str})\n",
    "\n",
    "# Rename columns in ratios file\n",
    "ratios_all.rename(columns = {'permno': 'PERMNO_r', 'public_date': 'DATE_r'}, inplace = True)\n",
    "\n",
    "# REMOVE DATA THAT IS (PBLY) NOT NEEDED\n",
    "ratios_all = ratios_all.drop(['adate', 'qdate'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "#                                                    DATA PREPARATION                                         #\n",
    "###############################################################################################################\n",
    "\n",
    "### DATE\n",
    "# set date as datetime64\n",
    "wharton['DATE_w'] = pd.to_datetime(wharton['DATE_w']).dt.date.astype('datetime64[ns]')\n",
    "ratios_all['DATE_r'] = pd.to_datetime(ratios_all['DATE_r']).dt.date.astype('datetime64[ns]')\n",
    "\n",
    "### PERMNO\n",
    "# set PERMNO column as integer (int64) for ratios_all, because its type 'O'\n",
    "ratios_all['PERMNO_r'] = ratios_all['PERMNO_r'].convert_objects(convert_numeric = True)\n",
    "\n",
    "### RENAME COLUMNS AND SET PERMNO AND DATE AS INDEX\n",
    "wharton.rename(columns = {'PERMNO_w': 'PERMNO', 'DATE_w': 'DATE'}, inplace = True)\n",
    "ratios_all.rename(columns = {'PERMNO_r': 'PERMNO', 'DATE_r': 'DATE'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_permno = pd.DataFrame()\n",
    "\n",
    "# Now follows a typical split-change-merge pattern to calculate the values for each PERMNO\n",
    "# For each permno in df, do:\n",
    "for df_key in wharton.groupby('PERMNO').groups:\n",
    "    permno_group = wharton.groupby('PERMNO').get_group(df_key)\n",
    "    permno_group['PRC_RET'] = np.log(permno_group['PRC']/ permno_group['PRC'].shift(1))\n",
    "    grouped_by_permno = pd.concat([grouped_by_permno, permno_group])\n",
    "wharton = grouped_by_permno\n",
    "\n",
    "# Delete unused variables\n",
    "del df_key, permno_group, grouped_by_permno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set 0, 1 for PRC_RET\n",
    "def set_mov(mydata):\n",
    "    if mydata['PRC_RET'] > 0:\n",
    "        return 'UP'\n",
    "    elif mydata['PRC_RET'] < 0:\n",
    "        return 'DOWN'\n",
    "    elif mydata['PRC_RET'] == 0:\n",
    "        return 'UP'\n",
    "\n",
    "wharton = wharton.assign(MOVEMENT = wharton.apply(set_mov, axis = 1))\n",
    "wharton['PRC_MOV'] = wharton['MOVEMENT'].factorize()[0]\n",
    "wharton = wharton.drop('MOVEMENT', axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all dates to the first day of the month\n",
    "wharton['DATE'] = wharton['DATE'].apply(lambda dt: dt.replace(day=1))\n",
    "ratios_all['DATE'] = ratios_all['DATE'].apply(lambda dt: dt.replace(day=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JOIN DATASETS, OUTER\n",
    "# Set index\n",
    "wharton = wharton.set_index(['PERMNO', 'DATE'])\n",
    "ratios_all = ratios_all.set_index(['PERMNO', 'DATE'])\n",
    "\n",
    "# join datasets\n",
    "joined_dataset = wharton.join(ratios_all, how = 'outer')\n",
    "\n",
    "# reset index\n",
    "joined_dataset = joined_dataset.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_permno = pd.DataFrame()\n",
    "\n",
    "# Group by PERMNO-Code and then remove first (0) row because it is nan in every PRC_RET group\n",
    "for df_key in joined_dataset.groupby('PERMNO').groups:\n",
    "    permno_group = joined_dataset.groupby('PERMNO').get_group(df_key)\n",
    "    permno_group = permno_group[1:]\n",
    "    grouped_by_permno = pd.concat([grouped_by_permno, permno_group])\n",
    "joined_dataset = grouped_by_permno\n",
    "\n",
    "# Delete unused variables\n",
    "del df_key, permno_group, grouped_by_permno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove percentages in row \"divyield\" and divide with 100 (so its decimal percentage) with string split\n",
    "joined_dataset['divyield'] = joined_dataset['divyield'].str.rstrip('%').astype('float')/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Column with lagged movement of prices, because we want to have lagged prices as response vector later\n",
    "# because we want to forecast prices of tomorrow, not of today with our model\n",
    "joined_dataset['NEXT_DAY_PREDICTION'] = joined_dataset['PRC_MOV']\n",
    "grouped_by_permno = pd.DataFrame()\n",
    "\n",
    "for df_key in joined_dataset.groupby('PERMNO').groups:\n",
    "    permno_group = joined_dataset.groupby('PERMNO').get_group(df_key)\n",
    "    permno_group['NEXT_DAY_PREDICTION'] = permno_group['NEXT_DAY_PREDICTION'].shift(-1)\n",
    "    permno_group = permno_group[pd.notnull(permno_group['NEXT_DAY_PREDICTION'])]\n",
    "    grouped_by_permno = pd.concat([grouped_by_permno, permno_group])\n",
    "joined_dataset = grouped_by_permno\n",
    "\n",
    "# Delete unused variables\n",
    "del df_key, permno_group, grouped_by_permno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make date numeric because RF can not use otherwise\n",
    "joined_dataset['DATE'] = joined_dataset['DATE'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################################################\n",
    "#                                                    RANDOM FOREST                                            #\n",
    "###############################################################################################################\n",
    "\n",
    "##### FIRST: RUN FOREST WITHOUT REMOVING FEATURES BELOW!\n",
    "##### SECOND: REMOVE FEATURES THAT ARE \"USELESS\"\n",
    "##### THIRD: RUN THE FOREST AGAIN AND SEE THE SCORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a lot of variables that were foud useless in the RF process\n",
    "joined_dataset = joined_dataset.drop(['debt_at', 'cash_conversion', 'at_turn', 'adv_sale', 'rect_turn',\n",
    "       'de_ratio', 'divyield', 'pretret_earnat', 'gpm', 'sale_nwc',\n",
    "       'pretret_noa', 'npm', 'intcov_ratio', 'efftax', 'roe', 'roa', 'PRC_MOV',\n",
    "       'sale_invcap', 'debt_ebitda', 'short_debt', 'debt_capital', 'NAICS',\n",
    "       'ocf_lct', 'int_totdebt', 'PERMNO', 'totdebt_invcap', 'equity_invcap',\n",
    "       'lt_debt', 'staff_sale', 'cash_debt', 'CFACPR', 'dltt_be','BID', 'pcf', 'SHROUT', 'ALTPRC', 'GProf', 'pe_op_dil', 'cfm',\n",
    "       'accrual', 'bm', 'lt_ppent', 'evm', 'ASK', 'curr_debt', 'int_debt',\n",
    "       'quick_ratio', 'inv_turn', 'roce', 'ASKHI', 'opmbd', 'ptpm',\n",
    "       'sale_equity', 'fcf_ocf', 'capital_ratio', 'aftret_eq', 'cash_lt',\n",
    "       'cash_ratio', 'pay_turn', 'curr_ratio', 'rect_act', 'opmad',\n",
    "       'debt_invcap', 'profit_lct', 'invt_act', 'intcov', 'aftret_invcapx',\n",
    "       'debt_assets', 'aftret_equity', 'SPREAD', 'rd_sale', 'CFACSHR'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill missing value with sklearn IMPUTER (fills with mean)\n",
    "imp = Imputer(missing_values=np.nan, strategy = ___FILLSTRAT___ , axis=0)\n",
    "imputed_dataset = pd.DataFrame(imp.fit_transform(joined_dataset))\n",
    "imputed_dataset.columns = joined_dataset.columns\n",
    "imputed_dataset.index = joined_dataset.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RANDOM FOREST ####\n",
    "\n",
    "# Extract labels of features\n",
    "labels_of_features = imputed_dataset.columns[:-1]\n",
    "# X1 is the feature matrix\n",
    "X1 = imputed_dataset.iloc[:, :-1]\n",
    "# y1 is the response vector\n",
    "y1 = imputed_dataset.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the train - test- split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.2, random_state = 0, stratify = y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is the approximately same percentage of '1' i both training and test response vector\n",
    "y1_train.sum() / y1_train.size\n",
    "y1_test.sum() / y1_test.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization with sklearn StandardScaler\n",
    "standard_scaler = preprocessing.StandardScaler().fit(X1_train)\n",
    "X1_train = standard_scaler.transform(X1_train)\n",
    "X1_test = standard_scaler.transform(X1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RANDOM FOREST FEATURE SELECTION ####\n",
    "\n",
    "my_rainy_forest = RandomForestClassifier(random_state = 1)\n",
    "my_rainy_forest.max_depth = 8\n",
    "my_rainy_forest.fit(X1_train, y1_train)\n",
    "\n",
    "# Check features for their importance for the prediction\n",
    "features_importances = my_rainy_forest.feature_importances_\n",
    "# sort features in line with their importance for the prediction\n",
    "indices = np.argsort(features_importances)[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best 15 features and delete features above and start the RF again\n",
    "i = 0\n",
    "n = 15\n",
    "for i in range(n):\n",
    "    print('{0:2d} {1:7s} {2:6.4f}'.format(i+1, labels_of_features[indices[i]], features_importances[indices[i]]))\n",
    "del i,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ASSESS RESULTS ####\n",
    "\n",
    "# Test prediction of y1 with the test feature matrix: gives the prediction vector\n",
    "prediction1 = my_rainy_forest.predict(X1_test)\n",
    "\n",
    "# Calculate percentage of of ones in the test response vector\n",
    "y_test_score = print('Ratio of Ones in the Test Set =  ' + str(y1_test.sum() / y1_test.size))\n",
    "# Just to be sure the ones are distributed ca.the same in test and train response vector, check this:\n",
    "y_train_score = print('Ratio of Ones in the Training Set =  ' + str(y1_train.sum() / y1_train.size))\n",
    "# Calculate precentage of ones predicted with the model\n",
    "prediction_score = print('Score of Prediction =  ' + str(prediction1.sum() / prediction1.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the score surplus above the test-set response vector score\n",
    "(prediction1.sum()/prediction1.size) - (y1_test.sum()/y1_test.size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
